{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"TDC 실습 정리","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNa0OSkcNdW9lLPgj60t/86"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 데이터 전처리\r\n","\r\n","이 데이터로 모델을 학습시키기 위해선 각종 전처리가 필요합니다.\r\n","\r\n","우선 csv 데이터에서 index와 # of Sunspots 데이터를 추출한 후\r\n","\r\n","index -> time / # of Sunsports -> series\r\n","\r\n","로 변환하여 계산할 예정입니다.\r\n","\r\n","series 값은 min-max normalize를 실시하여 0-1 사이의 값으로 변환하였습니다."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["time_step = []\r\n","sunspots = []\r\n","\r\n","with open('sunspots.csv') as csvfile:\r\n","  reader = csv.reader(csvfile, delimiter=',')\r\n","  next(reader)\r\n","\r\n","  for row in reader:\r\n","    ## sunspots과 time_step 추출\r\n","    sunspots.append(float(row[2]))\r\n","    time_step.append(int(row[0]))\r\n","\r\n","## 데이터 추출\r\n","series = np.array(sunspots)\r\n","min = np.min(series)\r\n","max = np.max(series)\r\n","series -= min\r\n","series /= max\r\n","\r\n","time = np.array(time_step)\r\n","\r\n","split_time = 3000 # 필요한 값 셋팅\r\n","# 데이터 Train / Validation Split\r\n","time_train = time[:split_time]\r\n","x_train = series[:split_time]\r\n","time_valid = time[split_time:]\r\n","x_valid = series[split_time:]"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["학습 데이터 구성을 위해 Sliding Window 기법을 사용하겠습니다.\r\n","\r\n","window 사이즈를 30, batch 사이즈를 32로 구성하여 학습 데이터를 구성하였습니다.\r\n"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\r\n","    dataset = tf.data.Dataset.from_tensor_slices(series)\r\n","    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\r\n","    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\r\n","    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\r\n","    dataset = dataset.batch(batch_size).prefetch(1)\r\n","    return dataset\r\n","\r\n","window_size = 30\r\n","batch_size = 32\r\n","shuffle_buffer_size = 1000\r\n","\r\n","train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\r\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## 네트워크 정의\r\n","\r\n","tf.keras.models.Sequential을 이용해 네트워크를 정의하겠습니다.\r\n","\r\n","Time Series Forecasting 에서 Sliding Window 기법으로 구성된 데이터의 특성을 고려하여, Conv1D, LSTM, Dense 레이어로 네트워크를 구성했습니다.\r\n","\r\n","<br>\r\n","\r\n","구성된 데이터는 연속적인 특성을 가지고 있습니다. 그렇기에 인접한 데이터 간의 연관성을 활용하여 다음 Sunspot을 예측하는데 사용할 수 있습니다.\r\n","\r\n","Conv1D 레이어는 인접한 데이터간의 위치 관계에서의 특징을 추출하는데 사용합니다.\r\n","\r\n","그래서 네트워크의 가장 첫 부분에만 사용을 하였습니다.\r\n","\r\n","<br>\r\n","\r\n","LSTM 레이어는 RNN 레이어를 응용한 레이어입니다.\r\n","\r\n","LSTM은 RNN 레이어처럼 순차적으로 들어오는 데이터를 처리하기에 적합하며, RNN에 비해서 정보가 장기적으로 전달된다는 특징을 가지고 있습니다.\r\n","\r\n","<br>\r\n","\r\n","데이터 처리와 마지막 출력 값의 형태를 맞추기 위하여 Dense 레이어를 활용하였습니다.\r\n","\r\n","주어진 데이터를 바탕으로 다음 Date의 Sunspot 값을 예측하기 때문에 마지막 Layer의 형태는 1로 고정해줍니다.\r\n","\r\n","그리고 분류가 아니라 예측이기에, 마지막 Dense 레이어의 Activation 함수는 지정하면 안됩니다.\r\n","\r\n","<br>\r\n","\r\n","저는 이렇게 3가지를 활용하여 네트워크를 구성하였으나, LSTM 혹은 Dense 만으로 네트워크를 구성하더라도 유의미한 성능이 나오므로 취향에 따라서 구성하시면 될 것 같습니다.\r\n","\r\n","자세한 내용은 아래에 코드로 남겨두겠습니다."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["tf.keras.backend.clear_session()\r\n","tf.random.set_seed(51)\r\n","np.random.seed(51)\r\n","\r\n","tf.keras.backend.clear_session()\r\n","dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\r\n","\r\n","\r\n","model = tf.keras.models.Sequential([\r\n","        tf.keras.layers.Conv1D(filters=60, kernel_size=10,\r\n","                             strides=1, padding=\"causal\",\r\n","                             activation=\"relu\", input_shape=[None, 1]),\r\n","        tf.keras.layers.LSTM(60, return_sequences=True),\r\n","        tf.keras.layers.LSTM(60, return_sequences=True),\r\n","        tf.keras.layers.Dense(30, activation=\"relu\"),\r\n","        tf.keras.layers.Dense(30, activation=\"relu\"),\r\n","        tf.keras.layers.Dense(1)\r\n","    ])\r\n","\r\n","model = tf.keras.models.Sequential([\r\n","        tf.keras.layers.LSTM(64, return_sequences=True),\r\n","        tf.keras.layers.LSTM(64, return_sequences=True),\r\n","        tf.keras.layers.Dense(128, activation=\"relu\"),\r\n","        tf.keras.layers.Dense(128, activation=\"relu\"),\r\n","        tf.keras.layers.Dense(1)\r\n","    ])\r\n","\r\n","model = tf.keras.models.Sequential([\r\n","        tf.keras.layers.Dense(32, activation=\"relu\"),\r\n","        tf.keras.layers.Dense(64, activation=\"relu\"),\r\n","        tf.keras.layers.Dense(128, activation=\"relu\"),\r\n","        tf.keras.layers.Dense(128, activation=\"relu\"),\r\n","        tf.keras.layers.Dense(1)\r\n","    ])\r\n","    \r\n","model = tf.keras.models.Sequential([\r\n","        tf.keras.layers.LSTM(64, return_sequences=True),\r\n","        tf.keras.layers.LSTM(64, return_sequences=True),\r\n","        tf.keras.layers.LSTM(64, return_sequences=True),\r\n","        tf.keras.layers.Dense(1)\r\n","    ])\r\n","\r\n","model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9), metrics=['mae'])\r\n","model.fit(train_set, epochs=50)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## Validation 평가\r\n","\r\n","아까 데이터 구성에서 빼둔 236건의 Validation 데이터가 있습니다.\r\n","\r\n","이 데이터를 활용하여 학습 된 모델의 성능을 평가합니다.\r\n","\r\n","<br>\r\n","\r\n","해당 MAE가 0.12 이하로 떨어진다면 모델 설계 및 학습을 잘 실시한 것 입니다.\r\n"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import math\r\n","\r\n","def model_forecast(model, series, window_size):\r\n","   ds = tf.data.Dataset.from_tensor_slices(series)\r\n","   ds = ds.window(window_size, shift=1, drop_remainder=True)\r\n","   ds = ds.flat_map(lambda w: w.batch(window_size))\r\n","   ds = ds.batch(32).prefetch(1)\r\n","   forecast = model.predict(ds)\r\n","   return forecast\r\n","\r\n","\r\n","window_size = 30\r\n","rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\r\n","rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\r\n","\r\n","result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\r\n","\r\n","print(result)\r\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### LR_SCHEDULE 찾기"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# 콜백을 추가하여 학습 속도를 조정해보자.\r\n","# epoch number에 따라 lr값이 변경되도록 함.\r\n","tf.keras.backend.clear_session()\r\n","tf.random.set_seed(51)\r\n","np.random.seed(51)\r\n","\r\n","train_set = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)\r\n","\r\n","# RNN - SimpleRNN, Lambda, Dense layers\r\n","model = tf.keras.models.Sequential([\r\n","  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\r\n","                      input_shape=[None]),\r\n","  tf.keras.layers.SimpleRNN(40, return_sequences=True),\r\n","  tf.keras.layers.SimpleRNN(40),\r\n","  tf.keras.layers.Dense(1),\r\n","  tf.keras.layers.Lambda(lambda x: x * 100.0)\r\n","])\r\n","\r\n","# callback 설정을 통해 learnning rate를 조절하기 위함.\r\n","lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\r\n","\r\n","optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\r\n","\r\n","model.compile(loss=tf.keras.losses.Huber(),\r\n","              optimizer=optimizer,\r\n","              metrics=[\"mae\"])\r\n","history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])\r\n","#그래프 통해서 lre 찾기\r\n","# epoch가 어떻게 설정되었는지 플롯해보기\r\n","lrs = 1e-8 * (10 ** (np.arange(100) / 20))\r\n","plt.semilogx(lrs, history.history[\"loss\"])\r\n","plt.axis([1e-8, 1e-3, 0, 300])\r\n","\r\n","# 이렇게 찾은 loss function 중 가장 안정적인 값 선택\r\n","tf.keras.backend.clear_session()\r\n","tf.random.set_seed(51)\r\n","np.random.seed(51)\r\n","\r\n","dataset = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)\r\n","\r\n","model = tf.keras.models.Sequential([\r\n","  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\r\n","                      input_shape=[None]),\r\n","  tf.keras.layers.SimpleRNN(40, return_sequences=True),\r\n","  tf.keras.layers.SimpleRNN(40),\r\n","  tf.keras.layers.Dense(1),\r\n","  tf.keras.layers.Lambda(lambda x: x * 100.0)\r\n","])\r\n","optimizer = tf.keras.optimizers.SGD(lr=5e-5, momentum=0.9)\r\n","\r\n","model.compile(loss=tf.keras.losses.Huber(),\r\n","              optimizer=optimizer,\r\n","              metrics=[\"mae\"])\r\n","history = model.fit(dataset,epochs=400)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["## 검사"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["forecast=[]\r\n","for time in range(len(series) - window_size):\r\n","  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\r\n","\r\n","forecast = forecast[split_time-window_size:]\r\n","results = np.array(forecast)[:, 0, 0]\r\n","\r\n","\r\n","plt.figure(figsize=(10, 6))\r\n","\r\n","plot_series(time_valid, x_valid)\r\n","plot_series(time_valid, results)\r\n","\r\n","tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()\r\n","\r\n","import matplotlib.image  as mpimg\r\n","import matplotlib.pyplot as plt\r\n","\r\n","#-----------------------------------------------------------\r\n","# Retrieve a list of list results on training and test data\r\n","# sets for each training epoch\r\n","#-----------------------------------------------------------\r\n","mae=history.history['mae']\r\n","loss=history.history['loss']\r\n","\r\n","epochs=range(len(loss)) # Get number of epochs\r\n","\r\n","#------------------------------------------------\r\n","# Plot MAE and Loss\r\n","#------------------------------------------------\r\n","plt.plot(epochs, mae, 'r')\r\n","plt.plot(epochs, loss, 'b')\r\n","plt.title('MAE and Loss')\r\n","plt.xlabel(\"Epochs\")\r\n","plt.ylabel(\"Accuracy\")\r\n","plt.legend([\"MAE\", \"Loss\"])\r\n","\r\n","plt.figure()\r\n","\r\n","epochs_zoom = epochs[200:]\r\n","mae_zoom = mae[200:]\r\n","loss_zoom = loss[200:]\r\n","\r\n","#------------------------------------------------\r\n","# Plot Zoomed MAE and Loss\r\n","#------------------------------------------------\r\n","plt.plot(epochs_zoom, mae_zoom, 'r')\r\n","plt.plot(epochs_zoom, loss_zoom, 'b')\r\n","plt.title('MAE and Loss')\r\n","plt.xlabel(\"Epochs\")\r\n","plt.ylabel(\"Accuracy\")\r\n","plt.legend([\"MAE\", \"Loss\"])\r\n","\r\n","plt.figure()"],"outputs":[],"metadata":{}}]}