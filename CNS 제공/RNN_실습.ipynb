{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_실습.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8pcnjotAhTs"
      },
      "source": [
        "# RNN 실습\n",
        "\n",
        "앞서 살펴본 예제를 바탕으로, 실제로 코드를 작성하는 실습을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_tQITCFA8XJ"
      },
      "source": [
        "## 데이터 준비\n",
        "\n",
        "필요한 라이브러리를 import하고, Sarcasm 데이터셋을 로드합니다.\n",
        "\n",
        "Sarcasm 데이터셋은 뉴스 헤드라인이 모인 데이터로,\n",
        "\n",
        "그 뉴스가 Sarcastic한, 즉 비꼬는 기사인지 분류하도록 되어있는 데이터입니다.\n",
        "\n",
        "데이터를 로드하고, 파싱해줍니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "그 후, 각종 parameter를 정의하고 데이터를 분리해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag2pBiQwA4Ia"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "# 다운로드\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json\n",
        "\n",
        "with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "# 파싱\n",
        "sentences = []\n",
        "labels = []\n",
        "urls = []\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "\n",
        "vocab_size = 1000 # 단어사전의 크기\n",
        "embedding_dim = 16 # 토큰 임베딩 차원 수\n",
        "max_length = 120 # 입력 시퀀스의 최대 길이를 지정\n",
        "trunc_type='post' # max_length보다 긴 시퀀스의 경우 어디를 잘라낼 것인지 지정. pre는 앞쪽을, post는 뒷쪽을 잘라냄\n",
        "padding_type='post' # max_length보다 긴 시퀀스의 경우 어디에 padding을 붙일 것인지 지정. pre는 앞쪽에, post는 뒷쪽에 붙임\n",
        "oov_tok = \"<OOV>\" # Out Of Vocabulary 토큰\n",
        "training_size = 20000 # trainings_size만 train 데이터로 사용, 나머지는 test 데이터로 사용\n",
        "\n",
        "train_sentences = sentences[0:training_size]\n",
        "test_sentences = sentences[training_size:]\n",
        "train_labels = labels[0:training_size]\n",
        "test_labels = labels[training_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yx8J1fYCS75"
      },
      "source": [
        "## **MISSION: 데이터 전처리**\n",
        "\n",
        "##**※ 실제 시험 문제가 이와 같은 형태로 출제됩니다.**\n",
        "\n",
        "위에서 로드하고 분리한 train_sentences와 test_sentences를 모델에 전달할 수 있는 형태로 변환해야합니다.\n",
        "\n",
        "##**작성할 코드**\n",
        "\n",
        "1. 학습 데이터를 기반으로 단어사전을 생성하세요.\n",
        "\n",
        "2. 생성한 단어사전을 이용해 데이터들을 정수 인덱스로 된 시퀀스로 변환하세요.\n",
        "\n",
        "3. 정수 시퀀스 형태로 변환된 데이터들을 padding 또는 truncating하여 길이를 맞춰주세요. 이때, padding은 뒤에서 하도록 설정해주세요.\n",
        "\n",
        "각종 함수들의 argument에는 위에서 정의한 parameter들을 활용하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrf4NSNGBYOX"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "## TODO: 크기가 vocab_size인 단어사전을 만듦\n",
        "\n",
        "\n",
        "## TODO: 위에서 만든 단어사전을 바탕으로 단어들을 정수 인덱스로 변환\n",
        "train_sequences = \n",
        "test_sequences = \n",
        "\n",
        "## TODO: pad_sequences 함수를 통해 padding 혹은 truncating하여 모든 데이터의 길이를 max_length로 통일\n",
        "# 길이가 짧은 데이터는 padding을 추가하고, 길이가 긴 데이터는 trunc_type 설정에 따라 잘라냄\n",
        "train_padded = \n",
        "test_padded = \n",
        "\n",
        "# numpy array 형태로 변환 (이 부분은 건드리지 마세요)\n",
        "train_padded = np.array(train_padded)\n",
        "test_padded = np.array(test_padded)\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLoT9lb5JfC3"
      },
      "source": [
        "## **MISSION: 네트워크 정의 및 학습**\n",
        "\n",
        "##**※ 실제 시험 문제가 이와 같은 형태로 출제됩니다.**\n",
        "\n",
        "위에서 변환한 데이터로 학습할 네트워크를 정의합니다.\n",
        "\n",
        "##**작성할 코드**\n",
        "\n",
        "아래의 학습 후 validation accuracy가 80%이상 나오도록 네트워크를 설계해보세요.\n",
        "\n",
        "<br>\n",
        "\n",
        "**더 좋은 성능을 가지는 네트워크 설계를 위한 팁**\n",
        "\n",
        "RNN Layer를 tf.keras.layers.Bidirectional()로 감싸면 Bidirectional RNN Layer를 만들 수 있습니다.\n",
        "\n",
        "※참조: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
        "\n",
        "<br>\n",
        "\n",
        "RNN Layer를 여러개 쌓는 경우, 마지막 RNN Layer를 제외한 다른 RNN Layer에\n",
        "\n",
        "return_sequence=True 옵션을 설정해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8uvtr6IBdev"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # TODO: Add Layers\n",
        "    \n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph7q66G6VHD_"
      },
      "source": [
        "## 모델 학습\n",
        "\n",
        "위에서 전처리한 데이터와 정의한 네트워크로 모델을 학습합니다.\n",
        "\n",
        "Validation accuracy가 80% 이상 나오도록 위의 네트워크를 설계해보세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHlXgh-uJB6i"
      },
      "source": [
        "num_epochs = 10\n",
        "model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels), verbose=1, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}