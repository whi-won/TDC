{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "RNN_예제.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 예제\n",
        "\n",
        "TensorFlow 2.0을 통해 자연어 데이터를 전처리하고, RNN을 사용하는 예제를 살펴봅니다."
      ],
      "metadata": {
        "id": "v9JDfOjgjXOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "845duCtjdWSn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np"
      ],
      "outputs": [],
      "metadata": {
        "id": "KticUNEmQCir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMDB 데이터셋 로드\r\n",
        "\r\n",
        "IMDB 데이터셋을 로드합니다.\r\n",
        "\r\n",
        "IMDB 데이터셋은 영어로 작성된 영화에 대한 리뷰를 모아놓은 데이터셋으로,\r\n",
        "\r\n",
        "리뷰의 내용에 따라 긍정적인 경우 1로, 부정적인 경우 0으로 label되어있습니다."
      ],
      "metadata": {
        "id": "gXe76aUDuGID"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\r\n",
        "\r\n",
        "train_data, test_data = imdb['train'], imdb['test']\r\n",
        "\r\n",
        "train_sentences = []\r\n",
        "train_labels = []\r\n",
        "\r\n",
        "test_sentences = []\r\n",
        "test_labels = []\r\n",
        "\r\n",
        "# 텍스트는 텍스트끼리, label은 label끼리 따로 분류\r\n",
        "for s, l in train_data:\r\n",
        "\r\n",
        "  # training_sentences.append(s.numpy().decode('utf8'))\r\n",
        "  \r\n",
        "  train_sentences.append(str(s.numpy()))\r\n",
        "  train_labels.append(l.numpy())\r\n",
        "  \r\n",
        "for s, l in test_data:\r\n",
        "  test_sentences.append(str(s.numpy()))\r\n",
        "  test_labels.append(l.numpy())\r\n",
        "  \r\n",
        "train_labels = np.array(train_labels)\r\n",
        "test_labels = np.array(test_labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "s_I2pJLrqtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 로드한 데이터 확인\r\n",
        "\r\n",
        "예시로 두 개의 데이터만 확인해보겠습니다.\r\n",
        "\r\n",
        "출력된 두 리뷰를 보면 첫 번째 리뷰는 부정적이고, 0으로 label되어있습니다.\r\n",
        "\r\n",
        "두 번째 리뷰는 긍정적이고, 1으로 label되어있습니다."
      ],
      "metadata": {
        "id": "Mke4hFnhvu6S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in [0, 14]:\r\n",
        "  print(train_labels[i], train_sentences[i])"
      ],
      "outputs": [],
      "metadata": {
        "id": "7huiYMGhquft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리\r\n",
        "\r\n",
        "이 데이터로 모델을 학습시키기 위해선 각종 전처리가 필요합니다.\r\n",
        "\r\n",
        "전처리에 사용할 함수들을 import하고, 각종 parameter를 설정합니다.\r\n",
        "\r\n",
        "그 후, 해당 parameter들을 바탕으로 전처리를 진행합니다.\r\n",
        "\r\n",
        "데이터 전처리가 어떻게 진행되는지 확인하기 위해 0번 데이터와 2번 데이터의 변화사항을 출력합니다.\r\n",
        "\r\n",
        "### Tokenizing\r\n",
        "1. Tokenizer instantiate\r\n",
        "2. fit_on_texts\r\n",
        "3. work_index : word index를 갖는 dictionary 생성\r\n",
        "4. texts_to_sequences : 각 문장의 벡터값들을 하나의 sequence로 합치기\r\n",
        "5. pad_sequences : 문장마다 길이가 다르므로 길이를 맞춰주기 위해서 0을 채움."
      ],
      "metadata": {
        "id": "U_gxPrVawCRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "vocab_size = 10000 # 단어사전의 크기\r\n",
        "embedding_dim = 16 # 토큰 임베딩 차원 수\r\n",
        "max_length = 120 # 입력 시퀀스의 최대 길이를 지정\r\n",
        "trunc_type='post' # max_length보다 긴 시퀀스의 경우 어디를 잘라낼 것인지 지정. pre는 앞쪽을, post는 뒷쪽을 잘라냄\r\n",
        "oov_tok = \"<OOV>\" # Out Of Vocabulary 토큰\r\n",
        "padding_type='post'\r\n",
        "\r\n",
        "print(\"원본 데이터 0번:\", train_sentences[0])\r\n",
        "print(\"원본 데이터 2번:\", train_sentences[2])\r\n",
        "print(\"#\" * 30)\r\n",
        "\r\n",
        "# 크기가 vocab_size인 단어사전을 만듦\r\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\r\n",
        "tokenizer.fit_on_texts(train_sentences)\r\n",
        "\r\n",
        "# 위에서 만든 단어사전을 바탕으로 단어들을 정수 인덱스로 변환\r\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\r\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\r\n",
        "\r\n",
        "print(\"정수로 인코딩 후 0번:\", train_sequences[0])\r\n",
        "print(\"길이:\", len(train_sequences[0]))\r\n",
        "print(\"정수로 인코딩 후 2번:\", train_sequences[2])\r\n",
        "print(\"길이:\", len(train_sequences[2]))\r\n",
        "print(\"#\" * 30)\r\n",
        "\r\n",
        "# pad_sequences 함수를 통해 padding 혹은 truncating하여 모든 데이터의 길이를 max_length로 통일\r\n",
        "# 길이가 짧은 데이터는 padding을 추가하고, 길이가 긴 데이터는 trunc_type 설정에 따라 잘라냄\r\n",
        "\r\n",
        "#  padding=padding_type 으로 옵션 주기도 한다.\r\n",
        "\r\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type)\r\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, truncating=trunc_type)\r\n",
        "\r\n",
        "print(\"Padding/Truncating 후 0번:\", train_padded[0])\r\n",
        "print(\"길이:\", len(train_padded[0]))\r\n",
        "print(\"Padding/Truncating 후 2번:\", train_padded[2])\r\n",
        "print(\"길이:\", len(train_padded[2]))"
      ],
      "outputs": [],
      "metadata": {
        "id": "CAx_LUiOwMdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력된 결과를 보면, 영어로 된 원본 텍스트 데이터가\n",
        "\n",
        "토크나이징 & 인코딩 후에는 정수의 list로 변환됩니다.\n",
        "\n",
        "길이는 각각 118, 133입니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "Padding/Truncating 후에는 길이가 120으로 통일되었습니다.\n",
        "\n",
        "길이가 118이던 0번 데이터는 padding으로 인해 앞에 0 두개가 추가되었습니다.\n",
        "\n",
        "길이가 133이던 2번 데이터는 truncating으로 인해 뒤쪽 13개가 삭제되었습니다."
      ],
      "metadata": {
        "id": "MkuCJY9R265H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 네트워크 정의\n",
        "\n",
        "tf.keras.models.Sequential을 이용해 RNN을 정의합니다.\n",
        "\n",
        "먼저, 임베딩 레이어가 필요합니다.\n",
        "\n",
        "임베딩 레이어는 단어사전 크기, 임베딩 크기, 시퀀스 길이를 argument로 받습니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "임베딩 후에 RNN Layer를 추가합니다.\n",
        "\n",
        "RNN Layer는 SimpleRNN, LSTM, GRU 등이 있습니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "그 후 Dense Layer를 추가합니다.\n",
        "\n",
        "일반적으로 RNN Layer 이후에 Dense Layer 하나를 먼저 추가하고,\n",
        "\n",
        "task에 맞는 node 개수를 가진 Output Dense Layer를 추가합니다.\n",
        "\n",
        "이진 분류 문제이므로 Output Layer에서는 sigmoid를 사용합니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "이진 분류 문제이므로 binary_crossentropy를 loss로 사용하여 모델을 컴파일합니다."
      ],
      "metadata": {
        "id": "BtSMKPFa3qlC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n",
        "    tf.keras.layers.LSTM(32),\r\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "\r\n",
        "'''\r\n",
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length), #임베딩 레이어 추가\r\n",
        "    tf.keras.layers.GlobalAveragePooling1D(), #Flatten보다 GlobalAveragePooling1D 사용하기\r\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "'''\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "j3EhAIauxzPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습\n",
        "\n",
        "학습 후 결과를 확인합니다."
      ],
      "metadata": {
        "id": "L2y4RbUl47fZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "num_epochs = 10\r\n",
        "history = model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels))"
      ],
      "outputs": [],
      "metadata": {
        "id": "Zq2i7xpOyEJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트\n",
        "\n",
        "직접 만든 테스트용 문구로 모델을 테스트합니다.\n",
        "\n",
        "첫 번째 문구에 대해서는 긍정인 1에 가까운 값을, 두 번째 문구에 대해서는 부정인 0에 가까운 값을 보입니다."
      ],
      "metadata": {
        "id": "0ygaEdamAdo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "my_sentences = [\"I cannot describe how great is this film in this narrow space.\",\r\n",
        "                \"Who is the director? Even I can make better than this.\"]\r\n",
        "\r\n",
        "my_sequences = tokenizer.texts_to_sequences(my_sentences)\r\n",
        "my_padded = pad_sequences(my_sequences, maxlen=max_length, truncating=trunc_type)\r\n",
        "\r\n",
        "model.predict(my_padded)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RKbYCIRwyQaU"
      }
    }
  ]
}